---
      
title: 随机森林
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

### 分类树与随机森林：学习文档

#### 第一部分：分类树简介

##### 1.1 分类树的背景与适用场景
在机器学习中，许多分类方法（如线性判别分析LDA和支持向量机SVM）通常假设数据是实数值或离散数值的特征向量，并且特征向量之间存在自然的距离度量（如欧氏距离）。然而，在某些应用场景中，分类问题涉及**名义数据（Nominal Data）**，即用于命名或标记变量而没有量化值的数据。例如，变量“交通方式”可能取值为“火车”、“汽车”、“飞机”、“海运”等。在这种情况下，传统的基于距离度量的分类器可能效果不佳。为了解决涉及名义数据的模式分类问题，可以采用**非度量方法（Nonmetric Method）**，如分类树（Classification Tree）。

##### 1.2 什么是分类树？
分类树是一种树结构的流程图，通过一系列问题对模式进行分类。其基本结构包括：
- **根节点（Root Node）**：位于树的顶部，包含整个数据集。
- **内部节点（Internal Nodes）**：表示对某个属性或特征的测试。
- **叶节点（Leaf Nodes）**：表示最终的类别标签，没有进一步的分支。
- **分支（Branches）**：表示基于测试结果的数据流向。

在分类过程中，从根节点开始，根据每个节点的问题（通常是“是/否”、“真/假”或“属性值是否属于某个集合”等形式），将数据逐步分流到不同的分支，最终到达叶节点并得到分类结果。

##### 1.3 分类树的优点
分类树的一个显著优点是**可解释性（Interpretability）**，体现在以下两个方面：
1. **决策过程直观**：对于任意测试数据，可以通过从根节点到对应叶节点的路径，轻松解释分类决策。例如，若特征为{味道、颜色、形状、大小}，数据 x = {甜、黄、细、中等}，分类结果为“香蕉”，因为满足 (颜色=黄) AND (形状=细)。
2. **融入专家知识**：分类树提供了一种自然的方式，将人类专家的先验知识融入模型设计中。

#### 第二部分：构建分类树的方法

##### 2.1 构建分类树的关键问题
使用训练数据构建分类树时，需要解决以下几个核心问题：
1. 节点的属性（特征）测试是否应限制为二值分裂（Binary Split），还是允许多值分裂（Multi-valued Split）？
2. 在某个节点上应选择哪个属性进行测试？
3. 何时将节点声明为叶节点？
4. 如果树变得“过大”，如何使其更小、更简单，即如何剪枝（Pruning）？
5. 如果叶节点不纯（Impure），如何分配类别标签？

##### 2.2 节点分裂的数量
每个节点的**分裂数量（Number of Splits）**与设计者设定的分支因子（Branching Factor）有关。分裂可以是：
- **二值分裂**：节点仅分为两个子节点。例如，属性 CP 有 4 个值 {1,2,3,4}，可以设计为“CP > 2？”（是/否）。
- **多值分裂**：节点分为多个子节点。例如，属性 CP 可以直接按值分裂为“CP=1”、“CP=2”、“CP=3”、“CP=4”。

在二值树（Binary Tree）中，所有节点都使用二值分裂。

##### 2.3 查询选择与节点不纯度
构建分类树的核心原则是**简单性（Simplicity）**：优先选择能生成简单、紧凑且节点少的树的决策。换句话说，优先选择能最好解释数据的简单模型。

为此，在每个节点 N 选择属性测试 T 时，目标是使得到达子节点的数据尽可能“纯”（Pure），即减少**不纯度（Impurity）**。不纯度的定义如下：
- 如果节点的所有样本属于同一类别，不纯度为 0；
- 如果节点样本的类别分布均匀，不纯度最大。

常用的不纯度度量方法有：
1. **熵不纯度（Entropy Impurity）**  
   定义为：  
  $i_E(N) = -\sum_j P(\omega_j) \log_2 P(\omega_j)$ 
   其中$P(\omega_j)$是节点 N 上属于类别$\omega_j$的样本比例。  
   示例：若节点 N 有两类样本，分别为 10 和 90，则$P(\omega_1) = 0.1, P(\omega_2) = 0.9$，熵不纯度为：  
  $i_E = -0.1 \log_2 0.1 - 0.9 \log_2 0.9 = 0.469$。  
   若类别分布为 50:50，则熵不纯度为 1，达到最大值。
   
2. **方差不纯度（Variance Impurity）**  
   特别适用于二分类问题，定义为：  
  $i_{Var}(N) = P(\omega_1) P(\omega_2)$。  
   示例：对于上述数据集，$i_{Var} = 0.1 \times 0.9 = 0.09$。
   
3. **基尼不纯度（Gini Impurity）**  
   方差不纯度的一般化形式，适用于多类别问题，定义为：  
  $i_{Gini}(N) = \frac{1}{2} \left(1 - \sum_j P^2(\omega_j)\right)$。  
   示例：对于上述数据集，$i_{Gini} = \frac{1}{2} (1 - 0.1^2 - 0.9^2) = 0.09$。
   
4. **误分类不纯度（Misclassification Impurity）**  
   定义为节点 N 上训练样本被误分类的最小概率：  
  $i_{Mis}(N) = 1 - \max_j P(\omega_j)$。  
   示例：$i_{Mis} = 1 - 0.9 = 0.1$。
   
熵不纯度因其计算简单且基于信息论而常被使用，基尼不纯度也广受关注。通常，不同不纯度度量的选择对结果影响不大。

##### 2.4 选择最优属性测试
给定部分树到节点 N，选择哪个属性及其值作为测试标准？直观的启发式方法是选择能最大程度降低不纯度的属性测试。不纯度下降定义为：  
$\Delta i(N) = i(N) - P_L i(N_L) - P_R i(N_R)$ 
其中$N_L$和$N_R$是左、右子节点，$i(N_L)$和$i(N_R)$是其不纯度，$P_L$和$P_R$是到达$N_L$和$N_R$的数据比例。

对于多值分裂，不纯度下降的扩展定义为：  
$\Delta i(N) = i(N) - \sum_{k=1}^B P_k i(N_k)$ 
其中 B 是分支数量，$P_k$是到达节点$N_k$的数据比例。

为避免过度偏好多分支分裂，可使用**增益比（Gain Ratio）**调整：  
$\Delta i'(N) = \frac{\Delta i(N)}{-\sum_{k=1}^B P_k \log_2 P_k}$ 
最佳测试值是使二值分裂的$\Delta i(N)$或多值分裂的$\Delta i'(N)$最大的选择。

##### 2.5 何时停止分裂
在训练二值树时，决定何时停止分裂是一个关键问题：
- 如果完全生长到每个叶节点对应最低不纯度，通常会导致过拟合（Overfitting）。极端情况下，每个叶节点对应一个训练样本，树仅作为查找表，无法泛化。
- 如果过早停止分裂，训练数据的错误率可能不足以降低，影响性能。

停止分裂的常见方法包括：
1. **交叉验证（Cross-Validation）**：用训练数据的子集构建树，剩余数据作为验证集，持续分裂直到验证集误差最小化。
2. **不纯度下降阈值**：若最佳候选分裂的不纯度下降小于预设阈值，则停止分裂。此方法允许直接使用所有训练数据，且叶节点可位于不同层次。
3. **样本数量阈值**：当节点样本数少于某个阈值（如 10 或总训练集的固定百分比）时停止分裂。
4. **权衡准则**：定义目标函数$J = \alpha \times size + \sum_{N \in leaf nodes} i(N)$，其中$size$是树复杂度的度量（如节点或链接数），$\alpha$是正常量，平衡树复杂度和训练数据性能。

##### 2.6 剪枝（Pruning）
剪枝是减少过拟合的重要步骤，涉及“修剪”树结构。方法是考虑所有相邻叶节点对（即连接到共同前驱节点的叶节点），若合并后不纯度增加较小，则合并这两个叶节点，并将共同前驱节点声明为叶节点。此过程可递归进行。

##### 2.7 叶节点标签分配
分配叶节点类别标签是树构建中最简单的步骤：
- 如果叶节点完全纯（不纯度为 0），直接分配唯一类别标签。
- 如果叶节点不纯（因提前停止分裂或剪枝），则分配样本最多的类别标签。
- 注意，极小的不纯度并不总是理想的，因其可能指示树对训练数据过拟合。

#### 第三部分：典型分类树算法

##### 3.1 CART（分类与回归树）
CART（Classification and Regression Tree）是本文讨论技术的核心，适用于分类和回归问题，使用基尼不纯度作为分裂标准，支持二值分裂。

##### 3.2 ID3（Iterative Dichotomiser 3）
ID3 算法通过迭代地将特征划分为两组或多组来构建树，仅适用于名义数据。若涉及实值变量，需先将其分桶（Binning）为离散区间。

##### 3.3 C4.5
C4.5 是 ID3 的改进版本，适用于实值变量（类似 CART），对名义数据使用多值分裂，并采用增益比不纯度进行查询选择，使用基于统计显著性的启发式方法进行剪枝。

##### 3.4 随机森林（Random Forest）
随机森林是一种基于 Bagging（Bootstrap Aggregation）的集成学习方法，通过构建多个分类树并集成结果来降低方差和过拟合。

#### 第四部分：随机森林详解

##### 4.1 动机
分类树通常采用贪婪算法（Greedy Algorithm），即在每个节点上优化分裂，而不考虑对整棵树的影响，易导致过拟合和高方差。为解决此问题，随机森林基于 Bagging 技术，通过对训练数据进行有放回的随机抽样（Bootstrapping），生成多个子数据集，每棵树在不同子数据集上训练，最终通过多数投票（Majority Voting）聚合结果。

##### 4.2 构建随机森林的步骤
假设训练数据集包含 N 个样本和 d 个特征：
1. 通过 Bootstrapping 创建 n 个数据子集（n < N）。
2. 对每个子集，从 d 个特征中随机选择 m 个特征（m < d），用这 m 个特征构建分类树。
3. 重复步骤 2，直到所有 n 个子集都用于训练。

##### 4.3 使用随机森林进行分类
1. 将测试样本通过 n 棵分类树ippets

Assistant: 每棵树运行测试样本，得到预测类别。
2. 计算每个预测类别的得票数。
3. 输出得票最多的预测类别作为最终分类结果。

##### 4.4 随机森林的特点
1. **多样性（Diversity）**：每棵树使用的特征不同，增强了模型的多样性。
2. **对维度灾难的免疫（Immune to Curse of Dimensionality）**：每棵树不考虑所有特征，减少了特征空间维度。
3. **并行化（Parallelization）**：每棵树独立构建，可并行计算。
4. **训练-测试划分（Train-Test Split）**：无需显式划分训练和测试数据，总有未被某棵树见过的数据（Out-of-Bag 数据）。
5. **稳定性（Stability）**：基于多数投票或平均，结果更稳定。

##### 4.5 随机森林与分类树的对比

| **方面**             | **随机森林**                       | **分类树**                     |
|----------------------|------------------------------------|--------------------------------|
| **性质**             | 多个分类树的集成                   | 单一分类树                     |
| **方差**             | 方差较低，减少过拟合               | 方差较高，易过拟合             |
| **准确性**           | 由于集成，通常较高                 | 易过拟合，准确性可能变化       |
| **鲁棒性**           | 对异常值和噪声更鲁棒               | 对异常值和噪声敏感             |
| **训练时间**         | 因构建多棵树，较慢                 | 构建单树，较快                 |
| **可解释性**         | 因集成，可解释性较差               | 单树，可解释性较强             |
| **适用场景**         | 适合复杂任务，高维数据             | 简单任务，易于解释             |

#### 第五部分：补充知识

##### 5.1 过拟合与泛化
过拟合（Overfitting）是指模型在训练数据上表现良好，但在新数据上表现较差的现象。分类树由于其贪婪算法特性，容易过拟合，而随机森林通过集成多个树降低了过拟合风险，提高了泛化能力（Generalization）。

##### 5.2 基尼不纯度的计算细节
基尼不纯度是一种直观的度量，计算复杂度较低，特别适合快速计算。公式中的$\sum_j P^2(\omega_j)$表示类别分布的“集中度”，值越大，分布越集中，不纯度越小。

##### 5.3 交叉验证的实现
交叉验证（如 k 折交叉验证）将数据分成 k 份，轮流用 k-1 份训练，1 份验证，评估模型性能，帮助选择停止分裂的时机。

#### 第六部分：例题与答案

##### 例题 1：熵不纯度计算
**问题**：某节点有两类样本，分别为 10 和 90，计算熵不纯度。
**答案**：  
$P(\omega_1) = 0.1, P(\omega_2) = 0.9$ 
$i_E = -0.1 \log_2 0.1 - 0.9 \log_2 0.9 = 0.469$

##### 例题 2：基尼不纯度计算
**问题**：某节点有两类样本，分别为 10 和 90，计算基尼不纯度。
**答案**：  
$P(\omega_1) = 0.1, P(\omega_2) = 0.9$ 
$i_{Gini} = \frac{1}{2} (1 - 0.1^2 - 0.9^2) = 0.09$

##### 例题 3：分类树节点选择
**问题**：在心脏病数据集中，特征“sex”的基尼不纯度为 0.2276，特征“fbs”的基尼不纯度为 0.2481，选择哪个特征作为根节点？
**答案**：选择“sex”，因为其基尼不纯度较低（0.2276 < 0.2481）。

##### 例题 4：连续变量分裂点选择
**问题**：对于连续变量“age”，分裂点候选值为 31.5、34.5、36 等，基尼不纯度最低为 0.228 时分裂点为 54.5，确定分裂点。
**答案**：选择分裂点为 54.5，因为其基尼不纯度最低。
