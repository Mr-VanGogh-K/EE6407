---
      
title: 聚类分析
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

# 聚类分析学习文档

## 1. 引言

在机器学习领域，之前我们讨论的学习方法多基于**有标签数据**，即监督学习（Supervised Learning），数据集中每个样本都带有明确的类别或目标值。然而，在现实世界中，许多数据是**无标签数据**，这促使了**无监督学习（Unsupervised Learning）**的发展。无监督学习不需要预先标记的训练数据集，是一种用于发现数据中潜在模式、结构或分组的技术。本文档将聚焦于无监督学习中的一个重要分支——**聚类分析（Clustering Analysis）**。

聚类分析是一种将数据点分组的技术，使得同一组（即簇，Cluster）内的数据点具有较强的相似性，而不同组之间的数据点相似性较低。聚类分析被广泛应用于市场细分、搜索结果分组、图像分割等领域。本文档将详细介绍聚类分析的基本概念、算法类型、具体实现方法以及聚类效果的评价指标。

---

## 2. 聚类分析基础

### 2.1 聚类分析的概念

聚类分析的目标是通过优化某个**准则函数（Criterion Function）**，将数据点划分为若干簇，使得簇内的数据点具有较高的相似性，而不同簇之间的数据点差异较大。换句话说，聚类过程试图找到数据的“自然分组”。

两个核心问题需要解决：
1. **什么是自然分组？** 自然分组通常指数据点在某些特征空间中形成的紧密聚集，簇内数据点之间的距离较小，而簇间距离较大。
2. **如何衡量相似性？** 簇内样本之间的相似性应显著高于簇间样本的相似性。

### 2.2 相似性度量

相似性度量的核心是计算数据点之间的“距离”，距离越小表示相似性越高。以下是常见的距离度量方法：

- **闵可夫斯基距离（Minkowski Distance）**  
  这是距离度量的一大类，公式为：
$$
  d(x, y) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{1/p}
$$
  其中$p$是可调参数：
  - 当$p = 2$时，为**欧几里得距离（Euclidean Distance）**，最常用的距离度量，表示两点之间的直线距离。
  - 当$p = 1$时，为**曼哈顿距离（Manhattan Distance）**，也称“城市街区距离”，表示沿坐标轴方向的路径距离。

- **马哈拉诺比斯距离（Mahalanobis Distance）**  
  考虑了数据特征之间的相关性，适用于数据分布不均匀的场景。公式为：
$$
  d(x, y) = \sqrt{(x - y)^T S^{-1} (x - y)}
$$
  其中$S$是数据的协方差矩阵。

- **余弦相似度（Cosine Similarity）**  
  通过计算两个向量之间的夹角余弦值来度量相似性，常用于文本数据等高维稀疏数据：
$$
  \text{similarity} = \cos(\theta) = \frac{x \cdot y}{\|x\| \|y\|}
$$

在聚类分析中，期望同一簇内的样本距离显著小于不同簇间的样本距离。

---

## 3. 聚类算法的类型

根据分组方式和策略的不同，聚类算法可分为以下几类：

### 3.1 基于中心的聚类（Centroid-based Clustering）

- **别名**：划分方法（Partitioning Methods）
- **特点**：将数据划分为非层次化的若干组，簇由中心向量（Centroid）表征，数据点被分配到最近的中心向量所在的簇。
- **代表算法**：K均值聚类（K-means Clustering）

### 3.2 层次聚类（Hierarchical Clustering）

- **特点**：通过构建树状结构（树状图，Dendrogram）来划分数据，不需要预先指定簇的数量。
- **方法**：
  - **凝聚法（Agglomerative，Bottom-up）**：从每个样本作为一个单独簇开始，逐步合并最相似的簇，直到所有样本合并为一个簇。
  - **分裂法（Divisive，Top-down）**：从所有样本作为一个簇开始，逐步分裂成更小的簇，直到每个样本为一个簇。

### 3.3 基于分布的聚类（Distribution-based Clustering）

- **特点**：假设数据服从某种分布（如高斯分布），基于数据点的概率分布进行分组。
- **代表算法**：高斯混合模型（Gaussian Mixture Model, GMM）

### 3.4 基于密度的聚类（Density-based Clustering）

- **特点**：将高密度区域连接成簇，支持任意形状的簇分布，通过低密度区域分隔不同簇。
- **代表算法**：DBSCAN（Density-Based Spatial Clustering of Applications with Noise）

以下将详细介绍每种聚类算法的原理与实现步骤。

---

## 4. 基于中心的聚类：K均值聚类

### 4.1 基本原理

K均值聚类是最简单且最常用的基于中心的聚类算法。其核心思想是：通过迭代优化，将数据划分为$k$个簇，每个簇由一个中心向量（Centroid）表示，数据点被分配到最近的中心向量所在的簇。

- **目标**：找到一个划分，使得簇内误差最小。
- **准则函数**：采用**簇内平方和（Within-Cluster Sum of Squares, WCSS）**作为优化目标，定义为：
$$
  WCSS = \sum_{i=1}^k \sum_{x \in C_i} \|x - m_i\|^2
$$
  其中$C_i$是第$i$个簇，$m_i$是簇$C_i$的中心（均值向量），$x$是簇内的样本点，$\|x - m_i\|^2$表示样本到中心的平方距离。

- **解释**：WCSS 衡量了将所有样本用$k$个簇中心表示时产生的总平方误差。最佳划分是使 WCSS 最小的划分。

- **适用场景**：K均值适用于簇呈紧凑球形且簇间分离较好的数据；对于非球形或密度不均的数据效果较差。

### 4.2 K均值算法步骤

1. **初始化**：设定簇的数量$k$，随机生成$k$个点作为初始簇中心（也可从数据中随机选择）。
2. **分配样本**：将每个数据点分配到距离最近的簇中心，形成$k$个簇。
3. **更新中心**：计算每个簇的新中心（均值向量）。
4. **迭代**：重复步骤 2 和 3，直到簇中心不再变化或达到最大迭代次数。

### 4.3 选择$k$值的方法：肘部法则（Elbow Method）

选择合适的簇数量$k$是 K均值聚类的关键问题。肘部法则是一种常用的方法：
1. 对不同$k$值运行 K均值聚类，计算每次的 WCSS。
2. 绘制$k$与 WCSS 的曲线。
3. 曲线的“肘部”（拐点）即为最佳$k$值，此时增加$k$对 WCSS 的减少效果不明显。

---

## 5. 层次聚类

### 5.1 基本原理

层次聚类通过构建树状结构（树状图，Dendrogram）来划分数据，不需要预先指定簇的数量。树状图展示了样本逐步合并或分裂的过程。

- **凝聚法（Agglomerative）**：从每个样本为一个单独簇开始，逐步合并距离最近的簇对，直到所有样本合并为一个簇。
- **分裂法（Divisive）**：从所有样本为一个簇开始，逐步分裂成更小的簇，直到每个样本为一个簇。

### 5.2 凝聚法算法步骤

1. 将每个样本视为一个单独簇（若有$n$个样本，则有$n$个簇）。
2. 计算所有簇之间的相似性（或距离），合并距离最小的两个簇，形成新簇（簇数量减 1）。
3. 更新新簇与其他簇的距离矩阵。
4. 重复步骤 2 和 3，直到所有样本合并为一个簇。
5. 绘制树状图，根据问题需求选择合适的簇数量。

### 5.3 簇间距离的计算方法

在凝聚法中，合并簇时需定义新簇与其他簇的距离，常见方法包括：
- **单链接（Single Linkage）**：新簇与其他簇的距离为两簇样本间的最小距离。
- **完全链接（Complete Linkage）**：新簇与其他簇的距离为两簇样本间的最大距离。
- **中心链接（Centroid Linkage）**：新簇与其他簇的距离为中心点间的距离。
- **平均链接（Average Linkage）**：新簇与其他簇的距离为两簇间所有样本对距离的平均值。

---

## 6. 基于分布的聚类：高斯混合模型（GMM）

### 6.1 基本原理

基于分布的聚类假设数据服从某种概率分布（如高斯分布），每个簇对应一个分布成分。GMM 是最常用的方法，核心思想是将数据建模为多个高斯分布的混合：
$$
p(x) = \sum_{i=1}^k \pi_i \cdot \mathcal{N}(x | \mu_i, \Sigma_i)
$$
其中$\pi_i$是第$i$个高斯成分的权重，$\mu_i$和$\Sigma_i$分别是均值向量和协方差矩阵，$\sum_{i=1}^k \pi_i = 1$。

### 6.2 算法步骤

1. 设定簇的数量$k$，随机初始化模型参数（均值、协方差、权重）。
2. 使用**期望-最大化（EM）算法**估计参数：
   - **E步**：计算每个数据点属于各簇的概率。
   - **M步**：基于概率更新模型参数。
3. 迭代 E步 和 M步，直到参数收敛。
4. 将数据点分配到概率最大的簇。

GMM 适用于数据分布接近高斯的场景，但在数据分布复杂或簇形状不规则时效果有限。

---

## 7. 基于密度的聚类：DBSCAN

### 7.1 基本原理

DBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一种基于密度的聚类方法，适用于任意形状的簇，并能识别噪声点。其核心思想是将高密度区域视为簇，低密度区域视为噪声。

- **关键参数**：
  - **ε**：邻域半径，用于确定一个点的邻域范围。
  - **MinPts**：最小点数阈值，邻域内至少需要多少点才能视为核心点。

- **数据点类型**：
  - **核心点（Core Point）**：邻域内点数 ≥ MinPts。
  - **边界点（Border Point）**：邻域内点数 < MinPts，但邻近核心点。
  - **噪声点（Noise/Outlier）**：既不是核心点也不是边界点。

- **密度可达性**：
  - **直接密度可达**：点$p$在核心点$q$的$ε$-邻域内。
  - **密度可达**：通过一系列核心点间接连接。
  - **密度连接**：两点通过某个核心点密度可达。

### 7.2 算法步骤

1. 设定参数$ε$和 MinPts，随机选择一个未访问的点作为起点。
2. 如果该点是核心点，创建新簇，递归找到所有密度可达的点并加入该簇。
3. 选择另一个未访问的点，重复步骤 2。
4. 遍历所有点，剩余未分配的点视为噪声。

### 7.3 优点与缺点

- **优点**：
  - 能识别任意形状的簇。
  - 对噪声点鲁棒。
  - 不需要预先指定簇数量。
- **缺点**：
  - 对密度变化较大的数据效果不佳。
  - 参数$ε$和 MinPts 的选择对结果影响较大。
  - 在高维数据上效率较低。

---

## 8. 聚类评价指标

聚类效果评价旨在量化聚类结果的好坏，常见指标包括：

### 8.1 轮廓系数（Silhouette Coefficient）

- 衡量每个样本点与其簇内其他点（凝聚性）及最近簇外点（分离性）的关系：
$$
  s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$
  其中$a(i)$是样本$i$到簇内其他点的平均距离，$b(i)$是到最近簇外点的平均距离。
- 范围：[-1, 1]，值越高表示聚类效果越好。

### 8.2 Dunn 指数（Dunn Index）

- 衡量簇间分离度和簇内紧凑度：
$$
  \text{Dunn Index} = \frac{\min_{i \neq j} d(C_i, C_j)}{\max_k \text{diam}(C_k)}
$$
  其中$d(C_i, C_j)$是簇间距离，$\text{diam}(C_k)$是簇内最大距离。值越高表示聚类越好。

### 8.3 Davies-Bouldin 指数（DB Index）

- 基于簇内分散度和簇间分离度的比值，值越小表示聚类效果越好。

### 8.4 Calinski-Harabasz 指数（CH Index）

- 衡量簇内凝聚性和簇间分离性的比值，值越高表示聚类越好。

### 8.5 Rand 指数（Rand Index）

- 衡量两种聚类方法的一致性，基于样本对在两种聚类中的分配情况，范围 [0, 1]，值越接近 1 表示一致性越高。

---

## 9. 例题与答案

以下是从文档中提取的例题及其答案，便于学习和复习。

### 例题 1：层次聚类中的距离矩阵与合并过程

**问题**：给定 6 个样本 S1 到 S6，其特征值如下，计算欧几里得距离矩阵，并使用单链接方法进行层次聚类，绘制树状图。

| 样本 | A1      | A2    | A3     | A4      |
|------|---------|-------|--------|---------|
| S1   | 1.2426  | 0.783 | -0.521 | -0.00342|
| S2   | 0.5079  | 1.107 | -1.212 | 2.48420 |
| S3   | 0.0716  | 1.479 | 0.999  | 1.04288 |
| S4   | 0.2323  | 0.231 | -1.074 | -0.18492|
| S5   | 0.2783  | 1.263 | 1.759  | 2.06782 |
| S6   | 0.0257  | 0.399 | 0.861  | 1.86497 |

**答案**：

1. **距离矩阵**（基于欧几里得距离）：
 $$
   \begin{array}{c|cccccc}
   & S1 & S2 & S3 & S4 & S5 & S6 \\
   \hline
   S1 & 0 & 2.704 & 2.294 & 1.290 & 3.263 & 2.651 \\
   S2 & 2.704 & 0 & 2.701 & 2.826 & 3.013 & 2.327 \\
   S3 & 2.294 & 2.701 & 0 & 2.718 & 1.311 & 1.365 \\
   S4 & 1.290 & 2.826 & 2.718 & 0 & 3.764 & 2.832 \\
   S5 & 3.263 & 3.013 & 1.311 & 3.764 & 0 & 1.288 \\
   S6 & 2.651 & 2.327 & 1.365 & 2.832 & 1.288 & 0 \\
   \end{array}
 $$

2. **合并过程**（使用单链接方法）：
   - 首次合并：S5 和 S6 距离最小（1.288），合并为 S56。
   - 第二次合并：S1 和 S4 距离最小（1.290），合并为 S14。
   - 第三次合并：S3 和 S56 距离最小（1.311），合并为 S356。
   - 第四次合并：S14 和 S356 距离最小（2.294），合并为 S13456。
   - 第五次合并：S13456 和 S2 距离为 2.327，合并为 S123456（最终簇）。

3. **树状图（Dendrogram）**：按照合并顺序绘制，高度反映合并时的距离。

---

### 例题 2：K均值聚类的轮廓系数评估

**问题**：有 300 个无标签样本，使用 K均值聚类，分别设定簇数量为 2、3、4、5，计算轮廓系数评估聚类效果。

**答案**：轮廓系数值如下（具体数值未在原文中给出，仅列出评估思路）：
- 簇数量$k=2$：轮廓系数较高，可能表示较好的聚类。
- 簇数量$k=3$：轮廓系数达到峰值，可能是最佳簇数量。
- 簇数量$k=4, 5$：轮廓系数下降，说明增加簇数量可能导致过拟合或簇间分离度下降。

最终选择轮廓系数最高的$k$值作为最佳簇数量。

---
