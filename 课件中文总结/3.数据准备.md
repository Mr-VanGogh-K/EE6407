---
      
title: 数据准备
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

# 机器学习数据准备中文学习文档

---

## 第一章：引言

### 1.1 机器学习概述
机器学习是一种通过数据和算法使计算机系统能够自动学习和改进的技术。根据学习方式的不同，机器学习可以分为以下三类：
- **监督学习**：通过已标记的训练数据（即包含输入和对应输出的数据）学习模型，进而对未知数据（测试数据）进行预测或分类。例如，预测房价、垃圾邮件分类等。
- **无监督学习**：在没有标记数据的情况下，尝试从数据中发现隐藏的模式或结构。例如，聚类分析、市场细分等。
- **强化学习**：通过与环境的交互，基于奖励和惩罚机制进行学习。例如，游戏AI、机器人控制等。

这些学习类型类似于人类的认知方式：监督学习如同在专家指导下学习，无监督学习如同自主发现规律，强化学习则类似于通过试错积累经验。

### 1.2 机器学习的应用
机器学习在多个领域有广泛应用：
- **银行与金融**：欺诈检测、信用评分。
- **保险**：风险预测、新客户评估。
- **医疗**：疾病预测、个性化治疗方案。

### 1.3 数据准备的重要性
数据是机器学习的基础，数据的质量和结构直接影响模型的效果。数据准备是机器学习项目中的关键步骤，包括数据的收集、探索、清洗和预处理。理解数据的性质和质量有助于选择合适的模型和算法，确保后续建模的准确性和效率。

---

## 第二章：机器学习活动流程

### 2.1 机器学习项目的主要活动
无论采用监督学习、无监督学习还是强化学习，机器学习项目通常包含以下核心步骤：
1. **数据获取与探索**：收集数据并初步分析其类型、质量和特征之间的关系。
2. **数据预处理**：对数据进行清洗、填补缺失值、处理异常值等操作，使其适合建模。
3. **数据划分（仅监督学习）**：将数据分为训练集（用于模型学习）和测试集（用于模型验证）。
4. **模型选择与训练**：选择合适的算法，基于训练数据训练模型（监督学习）；或直接在输入数据上应用无监督学习算法。
5. **模型评估与改进**：评估模型性能，调整参数或更换算法以提升效果。

数据准备是机器学习流程的起点，直接决定了后续步骤的成败。

---

## 第三章：数据的基本类型

### 3.1 数据集的构成
- **数据集**：关于某个实体或主题的相关信息集合。例如，学生数据集可能包含学生的学号、姓名、性别和年龄等信息。
- **样本**：数据集中的每一行，代表一个具体记录或观察。
- **属性/特征**：数据集中的每一列，描述样本的某一特定特性，也称为变量、维度或字段。例如，学生数据集中的“年龄”是一个属性。
- **数据空间**：如果一个数据集有n个属性，则称其为n维数据空间，每个样本是该空间中的一个点。

### 3.2 数据的分类
数据主要分为以下两类：
1. **定性数据（Qualitative/Categorical Data）**：描述对象的质量或特性，无法用数值衡量。例如，学生的表现评价（优秀、良好、差）。
   - **名义数据（Nominal Data）**：无序的类别数据，无法比较大小。例如，血型（A、B、O、AB）、性别（男、女）。
   - **有序数据（Ordinal Data）**：类别数据具有天然的顺序，可比较大小，但无法计算差值。例如，客户满意度（非常满意、满意、不满意）。
2. **定量数据（Quantitative/Numeric Data）**：描述对象的数量或大小，可用数值衡量。
   - **区间数据（Interval Data）**：数值数据，值之间的差值有意义，但无绝对零点。例如，温度（摄氏度），40°C并非20°C的两倍热。
   - **比率数据（Ratio Data）**：数值数据，具有绝对零点，可进行加减乘除运算。例如，身高、体重、年龄。

### 3.3 数据属性的离散与连续
- **离散属性**：只能取有限或可数无限个值。例如，学号（有限）、学生排名（可数无限）。特殊类型为二元属性，仅有两个值（如是/否）。
- **连续属性**：可取任意实数值。例如，价格、身高、体重。

一般来说，名义和有序数据是离散的，而区间和比率数据是连续的。

---

## 第四章：数据结构探索

### 4.1 数据探索的意义
数据探索是理解数据集结构和特征的重要步骤，帮助识别数值型和类别型属性，并为后续预处理奠定基础。数据探索通常借助数据字典（包含属性描述和类型的元数据）或工具函数完成。

### 4.2 数值数据的探索
#### 4.2.1 中心趋势度量
中心趋势用于描述数据的集中位置，常用指标包括：
- **均值（Mean）**：所有数据值的总和除以数据数量。例如，数据21, 89, 34, 67, 96的均值为61.4。
- **中位数（Median）**：数据按顺序排列后的中间值。上述数据的有序列表为21, 34, 67, 89, 96，中位数为67。
- **众数（Mode）**：出现频率最高的值。例如，数据13, 13, 13, 13, 14, 14, 16, 18, 21的众数为13。

均值对异常值敏感，而中位数更能反映数据的典型值。若均值与中位数差异较大，需进一步检查是否存在异常值。

#### 4.2.2 数据分布度量
- **方差（Variance）**：衡量数据值偏离均值的程度，公式为：
 $$
  \text{Variance} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
 $$
  其中$x_i$为数据值，$\bar{x}$为均值，$n$为数据数量。
- **标准差（Standard Deviation）**：方差的平方根，直观反映数据分散程度。

#### 4.2.3 数据值位置度量
将数据按升序排列后，可定义以下五个关键值：
- 最小值（Minimum）
- 第一四分位数（Q1）：前25%数据的中间值
- 中位数（Q2）：中间50%数据的中间值
- 第三四分位数（Q3）：后25%数据的中间值
- 最大值（Maximum）

四分位距（IQR = Q3 - Q1）用于识别异常值。

#### 4.2.4 数值数据可视化
- **箱线图（Box Plot）**：展示数据的五数概括（最小值、Q1、中位数、Q3、最大值），并标记异常值。箱体表示IQR，须（Whisker）延伸至1.5倍IQR范围内的最远数据点，超出此范围的值为异常值。
- **直方图（Histogram）**：展示数据分布，将数据值划分为多个区间（bin），每个区间的条形高度表示数据频率。直方图可呈现对称、偏态或多峰分布。

### 4.3 类别数据的探索
类别数据探索选项较少，主要关注：
- 唯一值数量：例如，Auto MPG数据集中的“car name”有多少唯一值。
- 频率分布：统计每个类别的出现次数。

### 4.4 变量间关系的探索
- **散点图（Scatter Plot）**：用于可视化两个数值变量间的关系。将一个变量映射到x轴，另一个映射到y轴，每个点代表一个样本的坐标。例如，Auto MPG数据集中，“displacement”与“mpg”呈负相关。
- **二维交叉表（Cross-Tabulation）**：用于探索两个类别变量间的关系，以矩阵形式展示双变量频率分布。例如，统计Auto MPG数据集中不同“model.year”和“origin”的车辆数量。

---

## 第五章：数据质量与修复

### 5.1 数据质量问题
数据质量对机器学习模型的性能至关重要，常见问题包括：
1. **缺失值（Missing Values）**：数据记录中某些属性值为空。
2. **异常值（Outliers）**：数据值异常偏离其他值，可能影响模型预测。

数据质量问题可能由以下原因引起：
- **样本选择错误**：样本不具代表性，例如节日期间的销售数据无法预测日常销售。
- **数据收集错误**：人为记录错误或单位错误导致异常值；受访者拒绝回答导致缺失值。

### 5.2 数据修复策略
#### 5.2.1 处理异常值
异常值处理方法包括：
- **移除异常值**：如果异常值数量较少，可直接删除对应记录。
- **插补（Imputation）**：用均值、中位数、众数或其他相似样本的值替代异常值。
- **封顶（Capping）**：将异常值限制在一定范围内，例如用第5百分位数和第95百分位数值替换超出1.5倍IQR的值。
- **分开建模**：若异常值数量多且为自然现象，可将数据分为两部分分别建模。

#### 5.2.2 处理缺失值
缺失值处理方法包括：
- **删除记录**：若缺失值比例较小，可删除对应记录。例如，Auto MPG数据集中仅6条记录的“horsepower”值缺失，可删除后仍保留392条记录。
- **插补缺失值**：用均值、中位数或众数填补缺失值；或根据相似样本的属性值填补。例如，基于“cylinders”值填补“horsepower”缺失值。
- **估计缺失值**：根据距离函数找到相似样本，用其值填补缺失值。例如，根据年龄和身高相似的学生估计体重。

---

## 第六章：数据预处理

### 6.1 特征缩放、标准化与归一化
#### 6.1.1 特征缩放（Feature Scaling）的定义
特征缩放是一种数据预处理技术，用于将数据集中的特征值转换为相似尺度，确保所有特征对模型的贡献均衡，避免因数值范围差异导致某些特征主导模型。

#### 6.1.2 特征缩放的必要性
某些机器学习算法对特征尺度敏感，未缩放的数据会影响模型性能：
- **基于距离的算法**：如K近邻（KNN）、K均值聚类和支持向量机（SVM），因计算数据点间距离，易受特征尺度影响。例如，未缩放的数据可能导致收入特征（数值较大）比成绩特征（数值较小）对距离计算贡献更大。
- **基于梯度下降的算法**：如线性回归、逻辑回归，特征尺度差异会影响参数更新速度，导致收敛变慢。

#### 6.1.3 归一化（Normalization）
归一化是将数据值缩放到[0, 1]范围内的技术，也称为Min-Max缩放，公式为：
$$
x' = \frac{x - x_{min}}{x_{max} - x_{min}}
$$
其中$x_{min}$和$x_{max}$分别为特征的最小值和最大值。

#### 6.1.4 标准化（Standardization）
标准化是将数据转换为均值为0、标准差为1的分布，公式为：
$$
x' = \frac{x - \mu}{\sigma}
$$
其中$\mu$为均值，$\sigma$为标准差。标准化后的数据无固定范围限制。

#### 6.1.5 归一化与标准化的选择
归一化与标准化的选择取决于具体问题和算法。建议尝试原始数据、归一化数据和标准化数据，比较模型性能以确定最佳方法。

### 6.2 降维（Dimensionality Reduction）
高维数据集（特征数量多）需要更多计算资源，且并非所有特征都对模型有用。降维通过创建新特征（结合原始特征）来减少数据集维度，常见方法包括：
- **主成分分析（PCA）**：将相关变量转换为一组不相关的变量（主成分），主成分是原始变量的线性组合，彼此正交，捕捉数据最大变异。
- **奇异值分解（SVD）**：类似PCA，用于矩阵分解和降维。

降维可减少计算成本，提高模型性能，并增强模型可解释性。

### 6.3 特征子集选择（Feature Subset Selection）
特征选择旨在从所有特征中挑选最优子集，以降低计算成本，同时尽可能维持学习准确性。特征选择排除无关特征（对分类或聚类贡献小）和冗余特征（与其他特征信息重复）。

---

## 第七章：总结
1. 数据集是样本记录的相关信息集合，样本为数据集的行，属性为列。
2. 数据分为定性数据（名义和有序）和定量数据（区间和比率）。
3. 中心趋势通过均值、中位数和众数度量；数据分布通过方差和标准差描述；数据位置通过五数概括（最小值、Q1、中位数、Q3、最大值）表示。
4. 数值数据探索常用箱线图和直方图；类别数据探索关注唯一值和频率；变量间关系通过散点图和交叉表可视化。
5. 数据质量问题（如缺失值和异常值）需通过删除、插补等方法修复。
6. 数据预处理包括特征缩放（归一化、标准化）、降维（PCA、SVD）和特征选择，以提高模型性能。

---

## 附录：例题与答案

以下是从文档中提取的例题及其答案，供学习和练习使用。

### 例题1：均值与中位数的计算
**问题**：计算数据21, 89, 34, 67, 96的均值和中位数。
**答案**：
- 均值 = (21 + 89 + 34 + 67 + 96) / 5 = 307 / 5 = 61.4
- 中位数：将数据按升序排列为21, 34, 67, 89, 96，中间值为67。

### 例题2：众数的计算
**问题**：计算数据13, 13, 13, 13, 14, 14, 16, 18, 21的众数。
**答案**：13出现的频率最高（4次），因此众数为13。

### 例题3：箱线图须的计算
**问题**：给定一组数据，Q1=73，Q2（中位数）=76，Q3=79，计算箱线图的须（Whisker）范围，并确定须的具体位置（假设低范围数据为70, 63, 60，高范围数据为82, 84, 89）。
**答案**：
- 四分位距（IQR） = Q3 - Q1 = 79 - 73 = 6
- 下须范围 = Q1 - 1.5 × IQR = 73 - 1.5 × 6 = 64，下须位置为70（最低值大于64）。
- 上须范围 = Q3 + 1.5 × IQR = 79 + 1.5 × 6 = 88，上须位置为84（最高值小于88）。
- 异常值：60和63（低于64），89（高于88）。

### 例题4：特征缩放对距离的影响
**问题**：给定三个学生的高中CGPA（0-5范围）和未来收入（千元），未缩放前和缩放后的距离计算如下，解释缩放的必要性。
- 未缩放前学生1与2的欧几里得距离：大
- 未缩放前学生2与3的欧几里得距离：小
- 缩放后学生1与2的欧几里得距离：平衡
- 缩放后学生2与3的欧几里得距离：平衡
**答案**：未缩放前，由于收入特征数值范围远大于CGPA，距离计算主要受收入主导，导致学生间距离不平衡，影响模型性能。缩放后，两个特征对距离的贡献均衡，模型更公平地考虑各特征。

---
