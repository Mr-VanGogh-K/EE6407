---
      
title: SVM支持向量机
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      

---

# 支持向量机（SVM）学习文档

## 1. 引言与动机

支持向量机（Support Vector Machine, 简称SVM）是一种强大的监督学习算法，用于分类和回归问题。本文档主要聚焦于线性SVM的分类任务。假设我们有$N$个训练样本，分为两类，类别标签$d$分别取值为$+1$或$-1$，表示两个不同的类别。我们的目标是找到一个线性决策面（即超平面），将两类数据尽可能清晰地分隔开。

### 1.1 分类目标与决策超平面
线性分类的目标是找到一个超平面$\mathbf{w}^T \mathbf{x} + b = 0$，其中$\mathbf{x}$是特征向量，$\mathbf{w}$是可调整的权重向量，$b$是偏置项。该超平面将特征空间划分为两个区域：
- 当$\mathbf{w}^T \mathbf{x} + b > 0$时，样本预测为类别$d = +1$；
- 当$\mathbf{w}^T \mathbf{x} + b < 0$时，样本预测为类别$d = -1$。

如下图所示（基于PPT中的示例），超平面将两类数据分隔开，类别$+1$和$-1$分别位于超平面的两侧。

### 1.2 分类的置信度
在预测新样本的类别时，样本距离决策超平面的远近会影响预测的置信度。例如：
- 样本点 A 距离超平面较远，我们对预测其类别为$+1$非常有信心；
- 样本点 C 距离超平面很近，虽然预测为$+1$，但稍微调整超平面就可能导致预测变为$-1$，置信度较低。

因此，理想的超平面不仅要正确分类数据，还要使数据点尽可能远离决策面，以提高预测的置信度。换句话说，我们希望找到一个最大化“间隔”（margin of separation）的超平面。

### 1.3 优选超平面
PPT中展示了两种可能的超平面（a）和（b）。相比之下，超平面（b）更优，因为数据点距离决策面的间隔更大。这意味着超平面（b）具有更高的泛化能力，能更好地应对噪声或新数据。

## 2. 问题形式化

### 2.1 间隔与最优超平面
间隔（margin of separation）是指超平面与距离它最近的数据点之间的距离。SVM 的目标是找到一个超平面，使该间隔最大化，此时该超平面称为**最优超平面**。

超平面到点的距离可以用判别函数$g(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b$表示。对于一个点$\mathbf{x}$，将其分解为超平面上的投影点$\mathbf{x}_p$和垂直于超平面的部分：
$$
\mathbf{x} = \mathbf{x}_p + r \frac{\mathbf{w}}{\|\mathbf{w}\|}
$$
其中$r$是点$\mathbf{x}$到超平面的距离，$\frac{\mathbf{w}}{\|\mathbf{w}\|}$是超平面的法向量方向。

代入判别函数：
$$
g(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b = \mathbf{w}^T \mathbf{x}_p + r \frac{\mathbf{w}^T \mathbf{w}}{\|\mathbf{w}\|} + b = g(\mathbf{x}_p) + r \|\mathbf{w}\|
$$
由于$\mathbf{x}_p$在超平面上，$g(\mathbf{x}_p) = 0$，因此：
$$
r = \frac{g(\mathbf{x})}{\|\mathbf{w}\|}
$$
$r$的符号表示点在超平面的正侧或负侧。

假设数据是线性可分的，我们可以通过缩放$\mathbf{w}$和$b$，使：
- 对于$d = +1$的样本，$\mathbf{w}^T \mathbf{x} + b \geq 1$；
- 对于$d = -1$的样本，$\mathbf{w}^T \mathbf{x} + b \leq -1$。

满足等号的点称为**支持向量**（support vectors），它们是距离超平面最近的点，也是最难分类的点。

支持向量到超平面的距离为：
- 对于$d = +1$的支持向量，$r = \frac{1}{\|\mathbf{w}\|}$；
- 对于$d = -1$的支持向量，$r = \frac{-1}{\|\mathbf{w}\|}$。

因此，两类支持向量之间的间隔（margin）为：
$$
\rho = \frac{2}{\|\mathbf{w}\|}
$$

### 2.2 优化目标
最大化间隔$\rho$等价于最小化权重向量的欧几里得范数$\|\mathbf{w}\|$。因此，SVM 的优化目标可以形式化为：
$$
\min J(\mathbf{w}) = \frac{1}{2} \mathbf{w}^T \mathbf{w}
$$
约束条件为：
$$
d^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq 1, \quad i = 1, 2, \dots, N
$$
其中$d^{(i)}$是样本$\mathbf{x}^{(i)}$的类别标签，取值为$+1$或$-1$。

这是一个带约束的优化问题，目标函数$J(\mathbf{w})$是凸函数，约束条件是线性的。

### 2.3 原问题与拉格朗日乘子法
为了求解上述带约束的优化问题，我们引入拉格朗日乘子法，构造拉格朗日函数：
$$
L(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2} \mathbf{w}^T \mathbf{w} - \sum_{i=1}^N \alpha_i \{ d^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) - 1 \}
$$
其中$\alpha_i \geq 0$是拉格朗日乘子。

对$\mathbf{w}$和$b$求偏导并置为零，得到最优性条件：
1.$\frac{\partial L}{\partial \mathbf{w}} = 0$推导出：
  $$
   \mathbf{w} = \sum_{i=1}^N \alpha_i d^{(i)} \mathbf{x}^{(i)}
  $$
2.$\frac{\partial L}{\partial b} = 0$推导出：
  $$
   \sum_{i=1}^N \alpha_i d^{(i)} = 0
  $$

根据 Karush-Kuhn-Tucker (KKT) 条件，在最优解处：
$$
\alpha_i \{ d^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) - 1 \} = 0
$$
这意味着只有支持向量（即满足约束等式的样本点）对应的$\alpha_i$不为零。

### 2.4 对偶问题
通过代入最优性条件，拉格朗日函数可以转化为对偶问题：
$$
\max Q(\boldsymbol{\alpha}) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j d^{(i)} d^{(j)} (\mathbf{x}^{(i)})^T \mathbf{x}^{(j)}
$$
约束条件为：
-$\alpha_i \geq 0, \quad i = 1, 2, \dots, N$
-$\sum_{i=1}^N \alpha_i d^{(i)} = 0$

对偶问题是一个二次规划（Quadratic Programming, QP）问题，可以用现有QP软件求解得到最优的$\alpha_i$。随后，最优权重向量为：
$$
\mathbf{w}^* = \sum_{i=1}^N \alpha_i d^{(i)} \mathbf{x}^{(i)}
$$
偏置$b^*$可以通过支持向量计算：
$$
b^* = \frac{1}{N_s} \sum_{\alpha_i > 0} [d^{(i)} - (\mathbf{w}^*)^T \mathbf{x}^{(i)}]
$$
其中$N_s$是支持向量的总数。

## 3. 线性不可分样本的最优超平面

### 3.1 软间隔（Soft Margin）
当数据线性不可分时，某些样本点可能违反约束条件$d^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq 1$。这种违反有两种情况：
1. 样本点落在间隔区域内，但仍在超平面的正确一侧；
2. 样本点落在超平面的错误一侧。

为此，我们引入松弛变量$\xi_i \geq 0$，修改约束条件为：
$$
d^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq 1 - \xi_i
$$
其中：
- 若$0 \leq \xi_i \leq 1$，样本落在间隔区域内但在正确一侧；
- 若$\xi_i > 1$，样本落在错误一侧。

松弛变量$\xi_i$衡量样本偏离理想可分条件的程度。

### 3.2 原问题与目标函数
对于线性不可分数据，SVM 的目标是找到一个超平面，既尽可能最大化间隔，又最小化分类错误。目标函数修改为：
$$
\min J(\mathbf{w}, \boldsymbol{\xi}) = \frac{1}{2} \mathbf{w}^T \mathbf{w} + C \sum_{i=1}^N \xi_i
$$
其中$C$是一个超参数，控制间隔最大化与分类错误之间的权衡。较大的$C$意味着对错误分类的惩罚更高，会尽量减少错误；较小的$C$则允许更多错误以获得更大的间隔。

约束条件为：
-$d^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) \geq 1 - \xi_i$
-$\xi_i \geq 0$

### 3.3 对偶问题
引入拉格朗日乘子$\alpha_i$和$\beta_i$，构造拉格朗日函数：
$$
L = \frac{1}{2} \mathbf{w}^T \mathbf{w} + C \sum_{i=1}^N \xi_i - \sum_{i=1}^N \beta_i \xi_i - \sum_{i=1}^N \alpha_i [d^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) - 1 + \xi_i]
$$
根据 KKT 条件，最优解满足：
-$\alpha_i [d^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) - 1 + \xi_i] = 0$
-$\beta_i \xi_i = 0$
-$\alpha_i \geq 0, \beta_i \geq 0$

对偶问题为：
$$
\max Q(\boldsymbol{\alpha}) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j d^{(i)} d^{(j)} (\mathbf{x}^{(i)})^T \mathbf{x}^{(j)}
$$
约束条件为：
-$0 \leq \alpha_i \leq C$
-$\sum_{i=1}^N \alpha_i d^{(i)} = 0$

最优权重向量仍为：
$$
\mathbf{w}^* = \sum_{i=1}^N \alpha_i d^{(i)} \mathbf{x}^{(i)}
$$
偏置$b^*$通过满足$0 < \alpha_i < C$的支持向量计算：
$$
b^* = \frac{1}{N_s} \sum_{0 < \alpha_i < C} [d^{(i)} - (\mathbf{w}^*)^T \mathbf{x}^{(i)}]
$$

## 4. 核支持向量机简介
对于线性不可分的数据，线性SVM 无法有效分类。核支持向量机（Kernel SVM）通过将数据映射到更高维空间，在高维空间中找到线性分界面。核技巧避免了直接计算高维特征，而是通过核函数（如径向基函数 RBF、多项式核等）计算样本间的内积。核 SVM 的具体内容将在后续课程（如 EE7207 神经网络与深度学习）中深入探讨。

---

## 例题与答案

以下是从 PPT 中提取的例题及其讨论或答案，用于帮助理解 SVM 的概念。

### 例题 1：线性可分样本的 SVM
**问题描述**：PPT 中给出了一个线性可分数据集的示例（图 26-27），两类样本（Class 1 和 Class 2）可以通过一个超平面分隔开。支持向量和分隔超平面已标出。

**讨论**：支持向量是距离超平面最近的点，决定了超平面的位置。SVM 通过最大化间隔找到最优超平面。

**答案**：最优超平面如图 27 所示，支持向量位于超平面两侧的边界上。

### 例题 2：选择最优超平面
**问题描述**：PPT 中给出了另一个线性可分数据集（图 28-31），提供了两个可能的超平面。
1. 哪个超平面更优？
2. 为什么？

**讨论**：
- 第一个超平面（图 30）在训练数据上有 1 个分类错误。
- 第二个超平面（图 31）在训练数据上无分类错误，且间隔更大。

**答案**：
1. 第二个超平面更优。
2. 原因：它在训练数据上实现了 0 错误，并且数据点到超平面的间隔更大，具有更好的泛化能力。

### 例题 3：线性不可分样本的 SVM
**问题描述**：PPT 中给出了一个线性不可分数据集的示例（图 42-50），两类样本（Class 1 和 Class 2）无法通过线性超平面完全分隔。支持向量和决策超平面已标出。

**讨论**：通过引入松弛变量和超参数$C$，软间隔 SVM 找到一个尽量减少分类错误的最优超平面。图 48-50 展示了支持向量和决策超平面，部分支持向量可能位于错误一侧或间隔内。

**答案**：最优超平面如图 50 所示，支持向量通过$\alpha > 0$确定，部分支持向量可能有$\xi_i > 0$。

---
