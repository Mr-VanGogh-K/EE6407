---
      
title: LDA线形判别
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

# 线性判别分析（LDA）学习文档

## 第一部分：LDA的基本概念与原理

### 1.1 LDA的背景与目标
线性判别分析（Linear Discriminant Analysis, LDA）是一种经典的监督学习方法，用于分类问题。其目标是通过构造一个线性判别函数（即一条直线或一个超平面）来分离不同类别的样本。对于两类分类问题，LDA试图找到一个最优的超平面，将两类样本尽可能分开；对于多类问题，则通过多个判别函数将样本投影到低维空间进行分类。

### 1.2 线性判别函数的定义
线性判别函数可以表示为：
$$g(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + w_0$$
其中：
-$\mathbf{w}$是权重向量（weight vector），决定超平面的方向；
-$w_0$是偏置或阈值（bias or threshold weight），决定超平面的位置；
-$\mathbf{x}$是输入特征向量。

对于两类问题，分类器的决策规则为：
- 若$g(\mathbf{x}) > 0$，则判定为类别$\omega_1$；
- 若$g(\mathbf{x}) < 0$，则判定为类别$\omega_2$；
- 若$g(\mathbf{x}) = 0$，则位于决策边界上。

### 1.3 超平面的几何性质
- **超平面的法向量**：假设$\mathbf{x}_1$和$\mathbf{x}_2$是决策边界上的两点，则：
 $$\mathbf{w}^T (\mathbf{x}_1 - \mathbf{x}_2) = 0$$
  表明$\mathbf{w}$是超平面$H$的法向量，垂直于超平面上任意向量。
- **决策区域**：超平面将特征空间划分为两个半空间，分别对应两个类别：
  - 决策区域$R_1$（对应$\omega_1$）：$g(\mathbf{x}) > 0$，$\mathbf{w}$指向该区域；
  - 决策区域$R_2$（对应$\omega_2$）：$g(\mathbf{x}) < 0$。
- **距离计算**：样本点$\mathbf{x}$到超平面的距离$r$为：
 $$r = \frac{g(\mathbf{x})}{\|\mathbf{w}\|}$$
  若$r > 0$，则$\mathbf{x}$在超平面正侧；若$r < 0$，则在负侧。
- **原点到超平面的距离**：原点到超平面的距离由偏置$w_0$决定：
 $$r = \frac{w_0}{\|\mathbf{w}\|}$$
  - 若$w_0 > 0$，原点在正侧；
  - 若$w_0 < 0$，原点在负侧；
  - 若$w_0 = 0$，超平面通过原点。

### 1.4 LDA的目标：投影分离
LDA假设同一类样本聚集在一个簇内，不同类样本聚集在不同簇内。通过将样本投影到超平面（由$\mathbf{w}$确定）上，LDA希望：
- 不同类别的投影点尽可能分开；
- 同一类别内的投影点尽可能集中。
为此，LDA通过优化权重向量$\mathbf{w}$实现类别间的最大分离。

## 第二部分：Fisher线性判别分析（两类问题）

### 2.1 样本均值与投影均值
对于类别$i$，其$d$维样本均值为：
$$\mathbf{m}_i = \frac{1}{n_i} \sum_{\mathbf{x} \in D_i} \mathbf{x}$$
投影后的均值为：
$$\tilde{m}_i = \frac{1}{n_i} \sum_{\mathbf{x} \in D_i} \mathbf{w}^T \mathbf{x} = \mathbf{w}^T \mathbf{m}_i$$
两类投影均值的差值为：
$$|\tilde{m}_1 - \tilde{m}_2| = |\mathbf{w}^T (\mathbf{m}_1 - \mathbf{m}_2)|$$
该值反映了类别间分离的程度，LDA希望其尽可能大。

### 2.2 类内散度与总散度
为衡量类别内的分散程度，定义类内散度（scatter）：
$$\tilde{s}_i^2 = \sum_{\mathbf{x} \in D_i} (\mathbf{w}^T \mathbf{x} - \tilde{m}_i)^2$$
总类内散度为：
$$\tilde{s}^2 = \tilde{s}_1^2 + \tilde{s}_2^2$$
LDA希望类别内的分散尽可能小。

### 2.3 Fisher准则函数
Fisher线性判别通过构造准则函数$J(\mathbf{w})$来优化$\mathbf{w}$，使其在类别间分离最大化的同时，类别内分散最小化：
$$J(\mathbf{w}) = \frac{|\tilde{m}_1 - \tilde{m}_2|^2}{\tilde{s}_1^2 + \tilde{s}_2^2}$$

### 2.4 散度矩阵的定义
- **类内散度矩阵**：
 $$\mathbf{S}_i = \sum_{\mathbf{x} \in D_i} (\mathbf{x} - \mathbf{m}_i)(\mathbf{x} - \mathbf{m}_i)^T$$
 $$\mathbf{S}_W = \mathbf{S}_1 + \mathbf{S}_2$$
- **类间散度矩阵**：
 $$\mathbf{S}_B = (\mathbf{m}_1 - \mathbf{m}_2)(\mathbf{m}_1 - \mathbf{m}_2)^T$$
将投影均值差和类内散度用矩阵形式表达后，Fisher准则函数可重写为：
$$J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}$$

### 2.5 优化问题的求解
上述准则函数是一个广义Rayleigh商（generalized Rayleigh quotient）。通过求解广义特征值问题：
$$\mathbf{S}_B \mathbf{w} = \lambda \mathbf{S}_W \mathbf{w}$$
若$\mathbf{S}_W$可逆，则可转化为标准特征值问题：
$$\mathbf{S}_W^{-1} \mathbf{S}_B \mathbf{w} = \lambda \mathbf{w}$$
直接求解可得：
$$\mathbf{w} = \mathbf{S}_W^{-1} (\mathbf{m}_1 - \mathbf{m}_2)$$
若$\mathbf{S}_W$接近奇异矩阵，可引入正则化技术：
$$\mathbf{w} = (\mathbf{S}_W + \beta \mathbf{I})^{-1} (\mathbf{m}_1 - \mathbf{m}_2)$$
其中$\beta$是一个小的正数（如0.001），$\mathbf{I}$为单位矩阵。

### 2.6 偏置$w_0$的确定
偏置$w_0$通常设定为使超平面通过两类均值的中点：
$$w_0 = -\frac{\mathbf{w}^T (\mathbf{m}_1 + \mathbf{m}_2)}{2}$$
若两类样本投影的分散程度不同，可调整$w_0$以提高分类性能。

## 第三部分：多类判别分析（Multiple Discriminant Analysis）

### 3.1 多类问题的扩展
对于$c$类问题，LDA将样本从$d$维空间投影到$c-1$维空间，通过$c-1$个判别函数实现分类：
$$g_i(\mathbf{x}) = \mathbf{w}_i^T \mathbf{x}, \quad i=1,2,\dots,c-1$$
定义权重矩阵$\mathbf{W} = [\mathbf{w}_1, \mathbf{w}_2, \dots, \mathbf{w}_{c-1}]$，投影可表示为：
$$\mathbf{g}(\mathbf{x}) = \mathbf{W}^T \mathbf{x}$$

### 3.2 多类散度矩阵
- **总均值向量**：
 $$\mathbf{m} = \frac{1}{n} \sum_{i=1}^c n_i \mathbf{m}_i$$
- **类内散度矩阵**：
 $$\mathbf{S}_W = \sum_{i=1}^c \mathbf{S}_i, \quad \mathbf{S}_i = \sum_{\mathbf{x} \in D_i} (\mathbf{x} - \mathbf{m}_i)(\mathbf{x} - \mathbf{m}_i)^T$$
- **类间散度矩阵**：
 $$\mathbf{S}_B = \sum_{i=1}^c n_i (\mathbf{m}_i - \mathbf{m})(\mathbf{m}_i - \mathbf{m})^T$$
- **总散度矩阵**：
 $$\mathbf{S}_T = \mathbf{S}_W + \mathbf{S}_B$$

### 3.3 多类Fisher准则
多类LDA的目标是找到$\mathbf{W}$，使类间散度与类内散度的比值最大化，以行列式作为散度度量：
$$J(\mathbf{W}) = \frac{|\tilde{\mathbf{S}}_B|}{|\tilde{\mathbf{S}}_W|} = \frac{|\mathbf{W}^T \mathbf{S}_B \mathbf{W}|}{|\mathbf{W}^T \mathbf{S}_W \mathbf{W}|}$$
求解方法为广义特征值问题：
$$\mathbf{S}_B \mathbf{w}_i = \lambda_i \mathbf{S}_W \mathbf{w}_i$$
$\mathbf{W}$的列向量为对应最大特征值的广义特征向量。

## 第四部分：LDA算法总结

### 4.1 两类LDA算法步骤
1. 计算两类样本的均值向量$\mathbf{m}_1$和$\mathbf{m}_2$：
  $$\mathbf{m}_i = \frac{1}{n_i} \sum_{\mathbf{x} \in D_i} \mathbf{x}$$
2. 计算类内散度矩阵$\mathbf{S}_W = \mathbf{S}_1 + \mathbf{S}_2$：
  $$\mathbf{S}_i = \sum_{\mathbf{x} \in D_i} (\mathbf{x} - \mathbf{m}_i)(\mathbf{x} - \mathbf{m}_i)^T$$
3. 计算权重向量$\mathbf{w}$和偏置$w_0$：
  $$\mathbf{w} = \mathbf{S}_W^{-1} (\mathbf{m}_1 - \mathbf{m}_2)$$
  $$w_0 = -\frac{\mathbf{w}^T (\mathbf{m}_1 + \mathbf{m}_2)}{2}$$
4. 构造判别函数并分类：
  $$g(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + w_0$$
   若$g(\mathbf{x}) > 0$，分类为$\omega_1$；若$g(\mathbf{x}) < 0$，分类为$\omega_2$。

### 4.2 注意事项
- 特征值方法求解的$\mathbf{w}$可能与直接计算方法不同，但方向一致，可通过调整符号确保类别1样本在正侧。
- 若样本分散程度不同，可调整$w_0$提高性能。

## 第五部分：补充知识

### 5.1 特征值与广义特征值问题
- **特征值问题**：$\mathbf{A} \mathbf{x} = \lambda \mathbf{x}$，$\mathbf{x}$为特征向量，$\lambda$为特征值。
- **广义特征值问题**：$\mathbf{A} \mathbf{x} = \lambda \mathbf{B} \mathbf{x}$，常用于LDA优化问题。
在MATLAB中，可使用 `eig` 函数求解特征值和特征向量。

### 5.2 正则化技术
当$\mathbf{S}_W$接近奇异矩阵时，通过引入正则化参数$\beta$避免数值问题：
$$\mathbf{w} = (\mathbf{S}_W + \beta \mathbf{I})^{-1} (\mathbf{m}_1 - \mathbf{m}_2)$$

## 第六部分：例题与答案

### 例题1：两类LDA问题
**问题描述**：给定两类样本的均值和类内散度矩阵如下：
-$\mathbf{m}_1 = \begin{bmatrix} 0.1083 \\ -0.0653 \end{bmatrix}$
-$\mathbf{m}_2 = \begin{bmatrix} 1.8945 \\ 2.9026 \end{bmatrix}$
-$\mathbf{S}_W = \begin{bmatrix} 228.9365 & 10.9883 \\ 10.9883 & 189.216 \end{bmatrix}$

求权重向量$\mathbf{w}$和偏置$w_0$，并写出判别函数$g(\mathbf{x})$。

**解答**：
1. 计算$\mathbf{m}_1 - \mathbf{m}_2$：
  $$\mathbf{m}_1 - \mathbf{m}_2 = \begin{bmatrix} 0.1083 - 1.8945 \\ -0.0653 - 2.9026 \end{bmatrix} = \begin{bmatrix} -1.7862 \\ -2.9679 \end{bmatrix}$$
2. 计算$\mathbf{w} = \mathbf{S}_W^{-1} (\mathbf{m}_1 - \mathbf{m}_2)$：
   首先求$\mathbf{S}_W^{-1}$（PPT中已直接给出结果）：
  $$\mathbf{w} = \begin{bmatrix} -0.0071 \\ -0.0153 \end{bmatrix}$$
3. 计算$w_0 = -\frac{\mathbf{w}^T (\mathbf{m}_1 + \mathbf{m}_2)}{2}$：
  $$\mathbf{m}_1 + \mathbf{m}_2 = \begin{bmatrix} 0.1083 + 1.8945 \\ -0.0653 + 2.9026 \end{bmatrix} = \begin{bmatrix} 2.0028 \\ 2.8373 \end{bmatrix}$$
  $$\mathbf{w}^T (\mathbf{m}_1 + \mathbf{m}_2) = (-0.0071)(2.0028) + (-0.0153)(2.8373) = -0.0142 - 0.0434 = -0.0576$$
  $$w_0 = -\frac{-0.0576}{2} = 0.0288$$
4. 判别函数为：
  $$g(\mathbf{x}) = -0.0071 x_1 - 0.0153 x_2 + 0.0288$$

### 例题2：三类LDA问题
**问题描述**：给定三类样本的均值和散度矩阵如下：
-$\mathbf{m}_1 = \begin{bmatrix} 0.1083 \\ -0.0653 \end{bmatrix}$,$\mathbf{m}_2 = \begin{bmatrix} 1.8945 \\ 2.9026 \end{bmatrix}$,$\mathbf{m}_3 = \begin{bmatrix} 3.0515 \\ -0.935 \end{bmatrix}$
-$\mathbf{S}_W = \begin{bmatrix} 301.4383 & 11.4665 \\ 11.4665 & 284.352 \end{bmatrix}$
-$\mathbf{S}_B = \begin{bmatrix} 439.7196 & -56.5992 \\ -56.5992 & 809.7615 \end{bmatrix}$

求权重矩阵$\mathbf{W}$和偏置$w_{01}, w_{02}$，并写出判别函数$g_1(\mathbf{x})$和$g_2(\mathbf{x})$。

**解答**：
1. 求解广义特征值问题$\mathbf{S}_B \mathbf{w} = \lambda \mathbf{S}_W \mathbf{w}$，使用MATLAB函数 `eig` 得到：
  $$\mathbf{W} = \begin{bmatrix} -0.0564 & -0.012 \\ -0.0101 & 0.0585 \end{bmatrix}$$
   特征值：
  $$\lambda = \begin{bmatrix} 1.4155 & 0 \\ 0 & 2.9127 \end{bmatrix}$$
2. 计算投影均值：
   - 对于$\mathbf{w}_1 = \begin{bmatrix} -0.012 \\ 0.0585 \end{bmatrix}$，投影均值分别为$\tilde{m}_{11} = -0.0051$,$\tilde{m}_{12} = 0.1470$,$\tilde{m}_{13} = -0.0913$
   - 对于$\mathbf{w}_2 = \begin{bmatrix} -0.0564 \\ -0.0101 \end{bmatrix}$，投影均值分别为$\tilde{m}_{21} = -0.0054$,$\tilde{m}_{22} = -0.1361$,$\tilde{m}_{23} = -0.1626$
1. 计算偏置：
   -$w_{01} = -\frac{\tilde{m}_{11} + \tilde{m}_{12}}{2} = -0.0719$
   -$w_{02} = -\frac{\tilde{m}_{21} + \tilde{m}_{22}}{2} = 0.0708$
2. 判别函数为：
  $$g_1(\mathbf{x}) = -0.012 x_1 + 0.0585 x_2 - 0.0719$$
  $$g_2(\mathbf{x}) = -0.0564 x_1 - 0.0101 x_2 + 0.0708$$

### 决策规则：
1. 若$g_1(\mathbf{x}) > 0$且$g_2(\mathbf{x}) < 0$，分类为类别2；
2. 若$g_2(\mathbf{x}) > 0$且$g_1(\mathbf{x}) < 0$，分类为类别1；
3. 若$g_1(\mathbf{x}) < 0$且$g_2(\mathbf{x}) < 0$，分类为类别3；
4. 若$g_1(\mathbf{x}) > 0$且$g_2(\mathbf{x}) > 0$，则比较大小：若$g_1(\mathbf{x}) > g_2(\mathbf{x})$为类别2，否则为类别1。

---
