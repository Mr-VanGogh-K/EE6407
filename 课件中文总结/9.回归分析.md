---
      
title: 回归分析
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

### 回归分析学习文档（基于ML-LectureNote8.pdf）

---

### 第一章：回归分析概述

#### 1.1 回归与分类的区别
在机器学习中，监督学习主要分为两类：分类和回归。分类问题的目标变量是离散的类别标签（如“正常”与“异常”），而回归问题的目标变量是连续的数值变量。回归分析的目标是通过输入特征（自变量）预测输出值（因变量），例如预测房价、工资或温度等。

#### 1.2 回归问题的应用场景
回归分析广泛应用于经济学、工程学、医学等领域，例如：
- 预测房价基于房屋面积和位置；
- 预测销售额基于广告投入；
- 预测患者的血压基于年龄和体重。

回归分析的核心在于找到自变量与因变量之间的数学关系，并利用这一关系进行预测。

---

### 第二章：简单线性回归与多元线性回归

#### 2.1 简单线性回归（Single Linear Regression）
简单线性回归，也称为单变量回归，是一种最基础的回归模型，假设自变量$x$与因变量$y$之间存在线性关系。其数学模型为：
$$
y = \theta_0 + \theta_1 x + \varepsilon
$$
其中：
-$y$：因变量（目标变量，如工资）；
-$x$：自变量（预测变量，如工作经验）；
-$\theta_0$：截距（直线在y轴上的交点）；
-$\theta_1$：斜率（表示自变量对因变量的影响程度）；
-$\varepsilon$：残差（误差项，表示模型未解释的部分）。

简单线性回归的目标是找到最优的$\theta_0$和$\theta_1$，使得预测值与真实值之间的误差最小化。

#### 2.2 多元线性回归（Multiple Linear Regression）
当自变量不止一个时，简单线性回归扩展为多元线性回归。其数学模型为：
$$
y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_m x_m + \varepsilon
$$
其中：
-$x_1, x_2, \cdots, x_m$：多个自变量；
-$\theta_0, \theta_1, \cdots, \theta_m$：模型参数（系数）。

多元线性回归的目标是通过多个自变量共同预测因变量，捕捉更复杂的现实问题关系。

#### 2.3 矩阵表示
假设有$N$个训练样本，每个样本有$m$个特征，则可以用矩阵形式表示多元线性回归：
$$
\mathbf{y} = \mathbf{X} \cdot \boldsymbol{\theta} + \boldsymbol{\varepsilon}
$$
其中：
-$\mathbf{y}$是$N \times 1$的目标变量向量；
-$\mathbf{X}$是$N \times (m+1)$的设计矩阵（包含一列全1，用于截距项）；
-$\boldsymbol{\theta}$是$(m+1) \times 1$的参数向量；
-$\boldsymbol{\varepsilon}$是$N \times 1$的误差向量。

矩阵形式便于推导和计算，尤其在处理大规模数据时。

---

### 第三章：普通最小二乘法（OLS）

#### 3.1 OLS的基本原理
普通最小二乘法（Ordinary Least Squares, OLS）是估计线性回归模型参数的经典方法。其目标是最小化预测值与真实值之间的平方误差和，即定义损失函数为：
$$
J(\boldsymbol{\theta}) = \frac{1}{2} \sum_{i=1}^{N} \varepsilon_i^2 = \frac{1}{2} (\mathbf{y} - \mathbf{X} \boldsymbol{\theta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\theta})
$$

通过对$J(\boldsymbol{\theta})$求导并令导数为零，可以得到参数估计的解析解：
$$
\hat{\boldsymbol{\theta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$
这一估计方法被称为普通最小二乘估计，因为它通过最小化平方误差来拟合数据。

#### 3.2 OLS的局限性
尽管OLS是一种简单且有效的参数估计方法，但它存在以下问题：
1. **高维数据问题**：当自变量数量远大于样本数量时，$\mathbf{X}^T \mathbf{X}$可能不可逆，导致OLS估计无法计算。
2. **过拟合风险**：OLS估计可能在训练数据上表现良好，但对测试数据的泛化能力较差。
3. **矩阵病态性**：如果$\mathbf{X}^T \mathbf{X}$接近奇异矩阵，计算结果会不稳定。

---

### 第四章：正则化方法

#### 4.1 岭回归（Ridge Regression）
为了解决OLS的局限性，岭回归引入了对参数的L2范数惩罚项，从而限制参数值过大，增强模型的泛化能力。岭回归的损失函数为：
$$
J_{\text{ridge}}(\boldsymbol{\theta}) = \frac{1}{2} (\mathbf{y} - \mathbf{X} \boldsymbol{\theta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\theta}) + \lambda \boldsymbol{\theta}^T \boldsymbol{\theta}
$$
其中，$\lambda$是正则化参数，用于控制惩罚强度。

通过求导并令导数为零，可得岭回归的参数估计：
$$
\hat{\boldsymbol{\theta}}_{\text{ridge}} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y}
$$
其中，$\mathbf{I}$是单位矩阵。岭回归通过加入$\lambda \mathbf{I}$项，确保矩阵可逆，同时缩小参数值，减少过拟合风险。

#### 4.2 套索回归（Lasso Regression）
套索回归（Lasso，Least Absolute Shrinkage and Selection Operator）是一种L1正则化方法，其损失函数为：
$$
J_{\text{lasso}}(\boldsymbol{\theta}) = \frac{1}{2} (\mathbf{y} - \mathbf{X} \boldsymbol{\theta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\theta}) + \lambda \sum_{j=1}^{m} |\theta_j|
$$
Lasso与岭回归的区别在于使用L1范数惩罚项，这使得部分参数可能被压缩到零，从而实现特征选择，生成稀疏模型。

Lasso没有解析解，通常使用数值优化算法（如迭代收缩阈值算法ISTA）求解。$\lambda$的选择对模型影响很大，较大的$\lambda$会导致更多参数为零。

#### 4.3 正则化的意义
正则化是机器学习中防止过拟合的重要技术。过拟合是指模型在训练数据上表现良好，但在测试数据上表现较差的现象。正则化通过在损失函数中加入惩罚项，限制模型复杂度，从而提高泛化能力。

---

### 第五章：模型评估指标

#### 5.1 均方误差（MSE）
均方误差（Mean Squared Error, MSE）计算预测值与真实值之间差值的平方平均值：
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$
MSE对大误差的惩罚更严重，适用于强调大偏差的场景。

#### 5.2 均方根误差（RMSE）
均方根误差（Root Mean Squared Error, RMSE）是MSE的平方根，与目标变量具有相同单位：
$$
\text{RMSE} = \sqrt{\text{MSE}}
$$
RMSE常用于比较不同模型的预测精度。

#### 5.3 平均绝对误差（MAE）
平均绝对误差（Mean Absolute Error, MAE）计算预测值与真实值之间差值的绝对值平均：
$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$
MAE对所有误差一视同仁，更容易解释。

#### 5.4 决定系数（R²）
决定系数（R-squared）衡量模型解释目标变量变异的能力：
$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}
$$
其中，$\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$是残差平方和，$\text{TSS} = \sum_{i=1}^{n} (y_i - \bar{y})^2$是总平方和。$R^2$的值在0到1之间，值越大表示模型解释能力越强。

#### 5.5 调整后的决定系数（Adjusted R²）
$R^2$的缺点是随着特征数量增加，其值不会下降。为解决这一问题，调整后的决定系数（Adjusted R-squared）引入了特征数量的惩罚：
$$
\text{Adjusted } R^2 = 1 - \frac{(1 - R^2)(n - 1)}{n - m - 1}
$$
其中，$n$是样本数量，$m$是特征数量。Adjusted R² 更适合用于模型选择。

#### 5.6 评估指标总结
- MSE、RMSE和MAE值越低，模型精度越高；$R^2$值越高，模型解释能力越强。
- RMSE比MSE更常用，因为其单位与目标变量一致。
- MSE是可微函数，常作为优化目标，但在解释上不如MAE直观。
- Adjusted R² 比 R² 更适合处理多特征模型，避免无关特征的干扰。
- 在比较模型精度时，RMSE优于$R^2$。

---

### 第六章：模型验证与假设检验

#### 6.1 线性回归的基本假设
线性回归模型的有效性依赖于以下假设：
1. **线性关系**：自变量与因变量之间存在线性关系，可通过散点图（实际值 vs 预测值）验证。
2. **残差正态性**：残差应服从正态分布，可通过直方图或Q-Q图检验。
3. **残差均值为零**：残差均值应接近0，表示模型无系统性偏差。
4. **同方差性（Homoscedasticity）**：残差的方差在所有自变量值上应相同，可通过残差 vs 预测值的散点图检查。若残差呈现漏斗状，则表明存在异方差性（Heteroscedasticity）。
5. **自变量多变量正态性**：自变量应服从多变量正态分布，可通过Q-Q图验证。

#### 6.2 模型验证实例
PPT中通过散点图、残差直方图、Q-Q图和残差 vs 预测值图验证了上述假设，具体图形表明：
- 线性关系假设成立；
- 残差近似服从正态分布，且均值接近0；
- 残差无明显异方差性，满足同方差假设。

---

### 第七章：非线性回归简介

#### 7.1 非线性回归的定义
当自变量与因变量之间的关系不是线性时，线性回归模型不再适用，非线性回归成为更合适的选择。非线性回归的通用模型为：
$$
y = f(\mathbf{x}, \boldsymbol{\theta}) + \varepsilon
$$
其中，$f(\mathbf{x}, \boldsymbol{\theta})$是任意非线性函数。

#### 7.2 非线性回归示例
常见的非线性模型包括：
1. **多项式模型**：$y = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 + \varepsilon$；
2. **有理模型**：$y = \frac{\theta_0 + \theta_1 x + \theta_2 x^2}{\theta_3 + \theta_4 x + \theta_5 x^2} + \varepsilon$。

#### 7.3 非线性回归的局限性
非线性回归在近年来因人工神经网络的兴起而受到较少关注。神经网络能够处理复杂的非线性关系，且在大数据场景下表现更优。若对非线性回归感兴趣，可参考相关网络资源。

---

### 第八章：例题与解答

#### 例题1：简单线性回归与OLS估计
**问题描述**：给定200个数据对，假设$y$和$x$之间存在线性关系$y = \theta_0 + \theta_1 x$。通过OLS方法估计参数$\theta_0$和$\theta_1$。
- 数据对数量：$N = 200$；
- 设计矩阵：$\mathbf{X} = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_{200} \end{bmatrix}$；
- 目标向量：$\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_{200} \end{bmatrix}$。

**解答**：
通过OLS公式$\hat{\boldsymbol{\theta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$，计算得到：
$$
\hat{\boldsymbol{\theta}} = \begin{bmatrix} 0.4433 \\ 2.0728 \end{bmatrix}
$$
即模型为：
$$
y = 0.4433 + 2.0728 x
$$
散点图显示该直线较好地捕捉了$x$和$y$之间的关系。

#### 例题2：噪声对参数估计的影响
**问题描述**：数据实际由$y = 0.5 + 2x + \varepsilon$生成，其中$\varepsilon$是均值为0的高斯噪声。测试噪声标准差分别为1、0.5、0.1和0时，OLS估计的参数值。
- 真实参数：$\theta_0 = 0.5, \theta_1 = 2.0$。

**解答**：
- 噪声标准差$\sigma_\varepsilon = 1.0$：估计值为$\hat{\boldsymbol{\theta}} = [0.4433, 2.0728]^T$；
- 噪声标准差$\sigma_\varepsilon = 0.5$：估计值为$\hat{\boldsymbol{\theta}} = [0.4717, 2.0364]^T$；
- 噪声标准差$\sigma_\varepsilon = 0.1$：估计值为$\hat{\boldsymbol{\theta}} = [0.4943, 2.0073]^T$；
- 无噪声$\sigma_\varepsilon = 0.0$：估计值为$\hat{\boldsymbol{\theta}} = [0.5, 2.0]^T$。

**结论**：噪声水平越高，参数估计误差越大；当噪声消失时，估计值与真实值一致。

#### 例题3：$R^2$值的解释
**问题描述**：PPT中给出了三个数据集的散点图及其$R^2$值分别为0.2071、0.5511和0.9719。解释这些值的含义。
**解答**：
-$R^2 = 0.2071$：模型仅解释了目标变量变异的20.71%，拟合效果较差；
-$R^2 = 0.5511$：模型解释了目标变量变异的55.11%，拟合效果中等；
-$R^2 = 0.9719$：模型解释了目标变量变异的97.19%，拟合效果非常好。

**结论**：$R^2$值的理想范围取决于具体应用领域，物理过程可能要求$R^2 > 0.9$，而社会科学可能接受$R^2 < 0.5$。

---
